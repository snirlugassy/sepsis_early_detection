{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f2af34-cb9a-4a6f-85ac-e1d38eb52420",
   "metadata": {},
   "source": [
    "LSTM example: https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1001ae63-1efb-42f5-9bf2-958aa0871638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import tqdm\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d880a9-2e8f-4988-9df6-3166816b1a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files in /home/student/data/train\n",
      "Found 10000 files in /home/student/data/test\n"
     ]
    }
   ],
   "source": [
    "class ICUSepsisDataset(torch.utils.data.Dataset):\n",
    "    features = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP','Resp', 'Age', 'Gender']\n",
    "    target = 'SepsisLabel'\n",
    "    physiological = ['HR', 'O2Sat', 'Temp', 'SBP', 'MAP', 'DBP','Resp']\n",
    "    def __init__(self, path):\n",
    "        self.files = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "        print(f'Found {len(self.files)} files in {path}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = pd.read_csv(self.files[i], sep='|')\n",
    "        if data[ICUSepsisDataset.features].isna().all(axis=0).any():\n",
    "            # some physiological feature is missing\n",
    "            # currently, just return None,None\n",
    "            # TODO: implement better imputation mechanism\n",
    "            return None, None\n",
    "\n",
    "        for f in ICUSepsisDataset.physiological:\n",
    "            _x = data[f][~data[f].isna()]\n",
    "            try:\n",
    "                interp = interp1d(_x.index, _x.values, fill_value='extrapolate', kind='nearest')\n",
    "                data[f] = interp(data[f].index)\n",
    "            except:\n",
    "                return None, None\n",
    "\n",
    "        # Remove the data after the first sepsis trigger\n",
    "        if (data[ICUSepsisDataset.target] == 1).any():\n",
    "            first_sepsis_idx = data[ICUSepsisDataset.target].idxmax()\n",
    "            data = data[:first_sepsis_idx+1]\n",
    "\n",
    "        X = data[ICUSepsisDataset.features].to_numpy()\n",
    "        y = data[ICUSepsisDataset.target].to_numpy()\n",
    "        \n",
    "        assert len(X) == len(y)\n",
    "        \n",
    "        if np.isnan(X).any() or np.isnan(y).any():\n",
    "            raise ValueError('DF has missing values', i, data)\n",
    "        return torch.from_numpy(X).float(), torch.from_numpy(y)\n",
    "        \n",
    "\n",
    "\n",
    "train_data_path = '/home/student/data/train'\n",
    "test_data_path = '/home/student/data/test'\n",
    "\n",
    "icu_train = ICUSepsisDataset(train_data_path)\n",
    "icu_test = ICUSepsisDataset(test_data_path)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(icu_train, shuffle=True, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(icu_test, shuffle=True, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "785772d7-c189-4189-9b91-1b4b318fc2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SepsisPredictionModel(\n",
      "  (lstm): LSTM(9, 100, num_layers=2, batch_first=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=2, bias=True)\n",
      "  )\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SepsisPredictionModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim=100):\n",
    "        super(SepsisPredictionModel, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size=hidden_dim, batch_first=True, num_layers=2)\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, 100), \n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.Linear(100, 20), \n",
    "            torch.nn.ReLU(), \n",
    "            torch.nn.Linear(20, 2)\n",
    "        )\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SepsisPredictionModel(input_size=len(ICUSepsisDataset.features))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5ac4b0-586d-40f0-b847-3c3b9e38a301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 9235/20000 [04:07<04:49, 37.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7d97a9ab772f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{EPOCHS+1}, Loss {L}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    142\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m def adamw(params: List[Tensor],\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "loss = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(EPOCHS):\n",
    "    with tqdm.tqdm(total=len(icu_train)) as pbar:\n",
    "        for x,y in icu_train:\n",
    "            if x is None or y is None:\n",
    "                continue\n",
    "            model.zero_grad()\n",
    "            y_prob = model(x)\n",
    "            L = loss(y_prob, y)\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "            pbar.update(1)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS+1}, Loss {L}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2acc7bf6-1687-46cc-b534-1b290e9de50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0059, -5.1343], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd00bc-4837-45b9-87d9-b5ca62f28edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for x,y in test_dataloader:\n",
    "    _y = model(x)\n",
    "    correct += sum(_y.argmax(dim=2).flatten() == y.flatten())\n",
    "    print(correct)\n",
    "    total += len(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b44ed-cad6-49bf-a422-3dcde84054b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(data_loader, model, sample_size=None):\n",
    "    if sample_size is None:\n",
    "        sample_size = len(data_loader)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    for x,y in data_loader:\n",
    "        i += 1\n",
    "        _y = model(x)\n",
    "        correct += sum(_y.argmax(dim=2).flatten() == y.flatten())\n",
    "        total += len(y.flatten())\n",
    "        i += 1\n",
    "        if i >= sample_size:\n",
    "            break\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "calc_accuracy(test_dataloader, model, sample_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a3fa3-fc9d-4366-8753-09b371261307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
